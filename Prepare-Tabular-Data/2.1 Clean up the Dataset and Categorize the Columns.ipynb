{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c44e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e7cf276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Use the sample exploration config file to define a YAML configuration (config) file\n",
    "path_to_yaml = os.path.join(os.getcwd(),'data_exploration_config.yml')\n",
    "try:\n",
    "    with open (path_to_yaml, 'r') as c_file:\n",
    "        config = yaml.safe_load(c_file)\n",
    "except Exception as e:\n",
    "    print('Error reading the config file')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79c715bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Add the code to your Jupyter notebook to ingest the config file you defined in the previous step \n",
    "# and create Python variables for the parameters you defined in the config file\n",
    "load_from_scratch = config['general']['load_from_scratch']\n",
    "save_raw_dataframe = config['general']['save_raw_dataframe']\n",
    "save_transformed_dataframe = config['general']['save_transformed_dataframe']\n",
    "remove_bad_values = config['general']['remove_bad_values']\n",
    "categorical = config['columns']['categorical']\n",
    "continuous = config['columns']['continuous']\n",
    "date = config['columns']['date']\n",
    "text = config['columns']['text']\n",
    "excluded = config['columns']['excluded']\n",
    "max_lat = config['bounding_box']['max_lat']\n",
    "min_long = config['bounding_box']['min_long']\n",
    "min_lat = config['bounding_box']['min_lat']\n",
    "max_lat = config['newark_bounding_box']['max_lat']\n",
    "min_long = config['newark_bounding_box']['min_long']\n",
    "min_lat = config['newark_bounding_box']['min_lat']\n",
    "geo_columns = config['geo_columns']\n",
    "input_csv = config['file_names']['input_csv']\n",
    "pickled_input_dataframe = config['file_names']['pickled_input_dataframe']\n",
    "pickled_output_dataframe = config['file_names']['pickled_output_dataframe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "027388a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the Airbnb dataset into a pandas DataFrame\n",
    "df = pd.read_pickle(pickled_input_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdf5101a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['neighbourhood_group', 'neighbourhood', 'room_type']\n",
      "Continuous columns: ['minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'latitude', 'longitude', 'price', 'id']\n",
      "Text columns: ['name', 'host_name']\n",
      "name                    16\n",
      "host_name               21\n",
      "last_review          10052\n",
      "reviews_per_month    10052\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#4. For each column category (categorical, continuous, and text), \n",
    "# determine an appropriate replacement value to replace missing values\n",
    "print('Categorical columns:',str(categorical))\n",
    "print('Continuous columns:',str(continuous+excluded))\n",
    "print('Text columns:',str(text))\n",
    "count_missing = (len(df) - df.count())\n",
    "print(count_missing[count_missing != 0])\n",
    "\n",
    "# For the name column we could use 'Anonymous' to fill in missing names and host names.\n",
    "# for reviews_per_month column, we could fill missing values with 0, assuming no reviews.\n",
    "# For missing last_review dates, if there are no reviews_per_month then, we could\n",
    "# fill missing values with \"January 1, 1970\", as a specific date that clearly indicates \n",
    "# the absence of a review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8c8371",
   "metadata": {},
   "source": [
    "# Think about the pros and cons of a fixed value versus a calculated value\n",
    "\n",
    "**Filling Missing Values with the Most Common Value (Fixed Value) for categorical**:\n",
    "   - **Pros**: \n",
    "     - Ease of implementation: It's straightforward to implement and doesn't require complex computations.\n",
    "     - Can work well for large datasets with minimal impact on overall accuracy.\n",
    "   - **Cons**:\n",
    "     - It may reduce accuracy, especially when dealing with smaller datasets. By imputing the most common value, you may lose some of the nuances in the data.\n",
    "     - It doesn't consider the relationship between the missing value and other features, potentially leading to biased results.\n",
    "\n",
    "   - **When to Use**: This approach is suitable for larger datasets where the impact on accuracy is minimal, or when the categorical feature isn't a critical determinant of the outcome.\n",
    "\n",
    "2. **Filling Missing Values with Random Sampling Using Probability Density Function (PDF)**:\n",
    "   - **Pros**:\n",
    "     - Preserves the shape of the probability distribution: This method can provide a more nuanced representation of the data and maintain the distribution's characteristics.\n",
    "     - Can be suitable for smaller datasets where preserving data distribution is crucial.\n",
    "\n",
    "   - **Cons**:\n",
    "     - Requires computations: Implementing random sampling based on PDF might be more computationally intensive than using the most common value.\n",
    "     - Introduces noise: The randomness introduced by this method can add noise to the data, which might be undesirable in some cases.\n",
    "\n",
    "   - **When to Use**: Random sampling based on PDF is more suitable when you have a smaller dataset and preserving the underlying data distribution is essential. However, you should be cautious about the potential introduction of noise, especially in larger datasets where data drift can occur.\n",
    "\n",
    "In general, the choice between these two methods should also consider the specific context of your data and the impact of imputation on downstream analyses or models. Additionally, there are other techniques such as using machine learning models to impute missing categorical values, which can be more sophisticated and might provide better results in some cases.\n",
    "\n",
    "Ultimately, the decision on how to handle missing categorical values should be made based on a combination of factors, including dataset size, the importance of the categorical feature, and the specific goals of your analysis or modeling.\n",
    "\n",
    "Filling missing values in numerical columns requires different approaches, and the choice of method depends on the nature of the data and the specific context of your analysis. Here are some common strategies for handling missing numerical values:\n",
    "\n",
    "1. **Mean or Median Imputation**:\n",
    "   - **Pros**:\n",
    "     - Simple and easy to implement.\n",
    "     - Preserves the overall central tendency of the data.\n",
    "   - **Cons**:\n",
    "     - May not be suitable if the data has outliers, as it can be sensitive to extreme values.\n",
    "     - Can lead to underestimation or overestimation of variability in the data.\n",
    "\n",
    "   - **When to Use**: Mean or median imputation is a reasonable choice when missing values are missing completely at random and the data does not have severe outliers.\n",
    "\n",
    "2. **Regression Imputation**:\n",
    "   - **Pros**:\n",
    "     - Provides a more accurate estimate of missing values by considering relationships with other variables.\n",
    "   - **Cons**:\n",
    "     - Requires more complex modeling, which can be computationally expensive.\n",
    "     - Assumes a linear relationship between variables, which may not always hold.\n",
    "\n",
    "   - **When to Use**: Regression imputation is useful when there are strong relationships between the missing variable and other variables in the dataset. It can provide more accurate imputations than simple mean or median imputation.\n",
    "\n",
    "3. **K-Nearest Neighbors (K-NN) Imputation**:\n",
    "   - **Pros**:\n",
    "     - Takes into account the similarity between data points, making it robust to outliers and non-linear relationships.\n",
    "   - **Cons**:\n",
    "     - Computationally intensive, especially for large datasets.\n",
    "     - The choice of the number of neighbors (k) can impact results.\n",
    "\n",
    "   - **When to Use**: K-NN imputation is useful when the data points are not independent, and imputed values should be close to the values of neighboring data points.\n",
    "\n",
    "4. **Multiple Imputation**:\n",
    "   - **Pros**:\n",
    "     - Provides a robust way to handle missing data by generating multiple imputed datasets and combining results.\n",
    "     - Accounts for uncertainty in imputed values.\n",
    "   - **Cons**:\n",
    "     - Can be computationally intensive and complex to implement.\n",
    "\n",
    "   - **When to Use**: Multiple imputation is suitable when you want to account for uncertainty and obtain more accurate imputations. It's particularly useful when missing data are not missing completely at random.\n",
    "\n",
    "5. **Domain-Specific Imputation**:\n",
    "   - **Pros**:\n",
    "     - Customized imputation methods based on domain knowledge may yield the most accurate results.\n",
    "   - **Cons**:\n",
    "     - Requires expertise in the specific domain.\n",
    "\n",
    "   - **When to Use**: Domain-specific imputation methods are valuable when you have a deep understanding of the data and can design imputation strategies tailored to the dataset's characteristics.\n",
    "\n",
    "The choice of imputation method should be made carefully, considering the nature of the data, the presence of outliers, the amount of missing data, and the goals of your analysis. It's often a good practice to explore the data and assess the impact of different imputation methods on your results before making a final decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "565b5c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Write a function or functions to substitute the appropriate replacement value for each missing value\n",
    "def fill_missing():\n",
    "    df[['host_name','name']] = df[['host_name','name']].fillna('Anonymous')\n",
    "    df['last_review'] = df['last_review'].fillna(pd.to_datetime('January 1, 1970'))\n",
    "    df['reviews_per_month'] = df['reviews_per_month'].fillna(0)\n",
    "\n",
    "fill_missing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cf33a1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum_nights                  min    1.000000e+00\n",
      "                                max    1.250000e+03\n",
      "number_of_reviews               min    0.000000e+00\n",
      "                                max    6.290000e+02\n",
      "reviews_per_month               min    0.000000e+00\n",
      "                                max    5.850000e+01\n",
      "calculated_host_listings_count  min    1.000000e+00\n",
      "                                max    3.270000e+02\n",
      "latitude                        min    4.049979e+01\n",
      "                                max    4.091306e+01\n",
      "longitude                       min   -7.424442e+01\n",
      "                                max   -7.371299e+01\n",
      "price                           min    0.000000e+00\n",
      "                                max    1.000000e+04\n",
      "id                              min    2.539000e+03\n",
      "                                max    3.648724e+07\n",
      "dtype: float64\n",
      "Count of invalid values {'minimum_nights': 0, 'number_of_reviews': 0, 'reviews_per_month': 0, 'calculated_host_listings_count': 0, 'latitude': 0, 'longitude': 0, 'price': 0, 'id': 0}\n",
      "{'neighbourhood_group': 5, 'neighbourhood': 221, 'room_type': 3}\n"
     ]
    }
   ],
   "source": [
    "# Invalid values\n",
    "# Continuous columns should be integers instead of floats except latitude and longitude\n",
    "print(df[continuous+excluded].agg(['min','max']).unstack())\n",
    "print('Count of invalid values',{col:pd.to_numeric(df[col], errors='coerce').isna().sum() \n",
    "                                 for col in continuous+excluded})\n",
    "print({category: df[category].nunique() for category in categorical})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3e54c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Write functions that identify and deal with invalid values\n",
    "def fix_invalid():\n",
    "    cats = ['minimum_nights','number_of_reviews','reviews_per_month','calculated_host_listings_count']\n",
    "    df[cats] = df[cats].astype('int')\n",
    "\n",
    "fix_invalid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c8b6d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Save the DataFrame to a pickle file\n",
    "df.to_pickle(pickled_output_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
